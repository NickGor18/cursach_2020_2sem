\chapter{Основные понятия и обзор литературы}\label{chap1}

\section{Теория управления по прогнозирующей модели}\label{MPC}
\normalsize{Управление по прогнозируемой модели(далее MPC, от англ. "Model Predictive Control") это продвинутый метод управления процессами, который используется для соответствующего набора ограничений. Оно используется в перерабатывающей индустрии, на химических заводах и нефтепереработке с 80-х годов ХХ-го века. Сейчас данный метод так же используется при калибровке энергетических систем и силовой электронике. В МРС в основном используются динамические модели процессов, наиболее часто линекйные импирические модели полученные системной индетефикацией. Главным приимуществом MPC является то, что он способен оптимизировать текущий временной отрезок, держа ввиду будущие интервалы.\\
	Модели, используемые в MPC, направлены на то, чтобы отражать поведение сложных динамических систем. Они предсказывают изменения в зависимых переменных моделируемых систем благодаря изменениям независимых переменных. Например если речь идет о химических процессах, независимые переменные, которые могут быыть добавлены контроллером чаще всего являются либо установками регулятоных PID-контроллевров, либо конечными контрольными элементами.
}
\paragraph{Теория в основе MPC}
\normalsize{
	В основе MPC лежит итеративная конечно-горизонтальная оптимизация модели производства. В момент времени $t$ производится выборка текущего состояния производства и вычисляется стратегия управления минимизацией затрат (с помощью алгоритма численной минимизации) для относительно короткого временного горизонта в будущем: $[t,t+T]$. В частности, онлайн-расчет или расчет «на лету» используются для изучения траекторий состояния, которые исходят из текущего состояния, и находят (посредством решения уравнений Эйлера – Лагранжа) стратегию минимизации затрат до времени $t+T$.\\
	Реализуется только первый шаг стратегии управления, затем снова производится выборка состояния производства, и вычисления повторяются, начиная с нового текущего состояния, получая новый элемент управления и новый прогнозируемый путь состояния. Горизонт прогнозирования продолжает смещаться вперед, и по этой причине MPC также называют контролем горизонта отступания. Хотя этот подход не является оптимальным, на практике он дал очень хорошие результаты. Было сделано много научных исследований, чтобы найти быстрые методы решения уравнений типа Эйлера – Лагранжа, понять глобальные свойства устойчивости локальной оптимизации MPC и в целом улучшить метод MPC. 
}
\paragraph{Основные принципы MPC}
\normalsize{
	Управление по прогнозирующей модели это многомерный управляющий алгоритм, который использует:
	\begin{itemize}
		\item внутреннюю динамическую модель процесса
		\item функцию затрат $J$ над отступающим горизонтом
		\item алгоритм оптимизации, минимизирующий функцию стоимости $J$ с использованием управления u
	\end{itemize}
	Примером для нелинейной функции стоимости для оптимизации может служить следующее уравнение
	\begin{center}
		$J = \sum_{i=1}^{N}\omega_{x_i} (r_i - x_i)^2 + \sum_{i=1}^{N}\omega_{u_i}\bigtriangleup {u_i}^2$
	\end{center}
	не нарушая установленных ограничений утверждаем, что:
	\begin{flushleft}
		$x_i: i-$тая контролируемая переменная\\
		$r_i: i-$тая эталонная переменная\\
		$u_i: i-$тая изменяемая переменная\\
		$\omega_{x_i}-$коэффицент, отражающий важность $x_i$\\
		$\omega_{u_i}-$коэффицент, тормозящий относительно большие изменения в $u_i$\\
	\end{flushleft}
}
\newpage
\section{Задачи оптимального управления}\label{MPC}
\normalsize{Задачи оптимального управления относятся к теории экстремальных задач, то есть задач определения максимальных и минимальных значений. Задачи эти, как и собственно сама теория оптимального управления, возникла в начале ХХ-го века в связи с практическими задачами, появившимися из-за развития новой техники в различных областях. Данные экстремальные задачи не укладывались в рамки классического вариационного счисления.
	В данной главе мы рассмотрим их, используя различные примеры. В целом решение подобных задач можно разбить на два этапа:
	\begin{enumerate}
		\item Постановка задачи
		\item Решение с использованием условий оптимальности
	\end{enumerate}
	Данные пункты содержат в себе сразу несколько подпунктов, так что сейчас мы перейдем от общего к частному.
}
\paragraph{Постановка задачи}
\hfill \break
\normalsize{Изначально у нас есть некоторое, условие, однако его недостаточно для решения задачи. Для начала проведем математическую постановку задачи. 
	Она в себя будет включать следующие факторы: математическую модель объекта управления, цель управления, ограничения на траекторию воздействия, управляющее воздействие и его длительность и т.д. Рассмотрим данные факторы подробнее.
}\\
\paragraph{Модели объекта}
\hfill \break
\normalsize{Построение модели зависит от типа рассматриваемой задачи и того, что необходимо в итоге получить. Могут быть использованы различные дифференциальные уравнения: обыкновенные дифференциальные уравнения, уравнения с последействием, стохастические уравнения, уравнения в частных производных и т.д.
	Для примера будем использовать обыкновенное дифференциальное уравнение: }\\
\begin{flushright}
	\normalsize{ $\dot x(t) = f(t,x(t),u),\dot x(t)=dx\dt, t_0 \le t \le T $ \hspace{4cm} (1)}
\end{flushright}
\normalsize{$u \in R^m-$управление, $x \in R^n$-фазовый вектор системы, $f \in R^n$-заданная функция, а $R^n$ – евклидово пространство размерность n. Придавая нашему управлению различные значения мы получаем различные состояния объекта, из которых мы и выбираем оптимальное. }
\paragraph{Критерий качества}
\normalsize{Управление системой (1) осуществляется для достижения некоторых целей, которые формально записываются в терминах минимизации по u функционалов J, определяемых управлением u и траекторией х, где
	\begin{flushright}
		$J= \int_{t_0}^{T}(F(t,x(t),u)dt)+ \varphi (T,x(T)) \rightarrow min$\hspace{4cm}(2)
	\end{flushright} 
	F и $\varphi$ – заданные скалярные функции. Задача (2) в общем виде называется задачей Больца. При $F$ = 0 её называют задачей Майера, а при $\varphi$ = 0 – Лагранджа.}
\paragraph{Ограничения на траекторию и ограничения на управление}
\normalsize{Иногда траектория не может принадлежать какой-либо части пространства $R^n$. В таких случаях указывают, что $x(t) \in G(t)$,при том,что $G(t)$-заданная область в $R^n$. В зависимости от типа ограничений выделяют различные классы задач управления, такие как задачи с фиксированными концами, свободным левым либо правым концом. Так же существуют задачи с подвижными концами. Иногда же ограничения имеют интегральный характер и выглядят следующим образом:
	\begin{center}
		$J = \int_{t_0}^{T} F(t,x(t),u)dt \le 0$
	\end{center}
}
\normalsize{Если в задачах (1),(2) начальное и конечное положение задано, моменты начала и конца движения свободны, функция $\varphi = 0$, а $F = 1$, то получаем задачу о переводе системы (1) из начального положения в конечное за минимально возможное время.
	Далее мы рассмотри ограничения на управление, а после перейдем к примеру.
	Ограничения могут быть двух типов
	\begin{itemize}
		\item Информационные
		\item Ограниченность ресурсов управления
	\end{itemize}
	Информационные ограничения на управление зависят от того, какая именно информация о системе (1) доступна при выработке управляющего воздействия. Если вектор $x(t)$ недоступен измерению, то оптимальное управление ищется в классе функций $u(t)$, зависящих только от $t$. В этом случае оптимальное управление называется программным. Если же вектор $x(t)$ известен точно, то оптимальное управление называется синтезом оптимального управления и ищется в классе функционалов $u(t,x_{t_0}^{t}).$ Здесь $x_{t_0}^{t}$ – вся траектория движения на отрезке $t_0 \le s \le t$.
	Ограничения, обусловленные ограниченностью ресурсов управления имеют вид $u(t) \in U(t)$,где $U(t)$  заданное множество из $R^m$.
}
\paragraph{Условия оптимальности}
\paragraph{Принцип максимума}
\normalsize{Для начала сформулируем условия оптимальности в общем случае.\\
	\textbf{Теорема}: Пусть $u^0 (t),x^0 (t),t\in T$, - оптимальные управление и траектория задачи
	\begin{center}
		$J(u)=\varphi (x(t^* ))+\int_{0}^{t^*}f_0 (x(t),u(t))dt \rightarrow min$
	\end{center}
	\begin{flushright}
		$\dot{x}=f(x,u), x(0)=x_0$ \hspace{6cm} (3)
	\end{flushright}
	\begin{center}
		$x(t^*) \in X^*=\{x \in R^n: h_i \le 0,i=1,...,m_1,h_i(x)=0,i=m_1+1,...,m\},
		u(t) \in U,t\in T=[0,t^* ]$, где $h_i(x))$-непрерывно дифференцируемые функции,
		$x\in R^n  i=1,...,m, m<n.$
	\end{center}
	Тогда найдутся такие числа $\lambda_i^0,i=1,...,m)$, что вдоль указанных управления $u^0 (t),t\in T$,траектории $x^0 (t),t\in T$,и решения $\psi^0 (t),t\in T$,сопряженной системы $x(t)^*\in X^*$ выполняются условия: 
}
\begin{center}
	\begin{enumerate}
		\item Условие нетривиальности: $\sum_{i=0}^{m} (\lambda_i^0)^2 \not= 0;$
		\item Условия неотрицательности: $\lambda_i^0 \ge 0, i=0,...,m_1;$
		\item Условие максимума: $H(x^{0}(t),\psi^0(t), u^0(t)) = max H(x^0(t),\psi^0(t), u(t)), t \in \[t,t^*\[, $ где максимум мы берем по $u$
		\item Условие трансверсальности: 
		\begin{center}
			$\psi^0 (t^*)=-\lambda_0^0   \partial\psi(x^0 (t^* ))\partial x-\sum_{i=0}^{m}\lambda_i^0   (\partial h_i (x^0 (t^* )))\partial x$
		\end{center}
		\item Условия дополняющей нежесткости: $\lambda_i^0 h_i (x^0(t^*))=0,i=1,...,m_1.$
	\end{enumerate}
\end{center}
Чтобы продолжить решение нам необходимо понять, что же такое условие максимума. Сформулируем теорему.\\
\textbf{Теорема}: Оптимальное управление управление  $u^0 (t), t\in T $в задаче   $J_p (u)=\varphi(x(t^* ))+\int_{0}^{t^*}f_0(x(t),u(t))dt+\sum_{(i=1)}^{m} \rho_i h_i^2 x(t^* ) \rightarrow min, \dot{x} =f(x,u),x(0)= x_0,  u(t)\in U,t\in T,$
где $\rho_i>0 $– штраф за «единицу» нарушения$ h_i^2 x(t^* )=1$ ограничения $h_i (x)=0$ вместе с соответствующей траекторией $x^0 (t),t\in T$, удовлетворяют условию максимума:$ H(x^0 (t),\psi^0 (t),u^0 (t))=\overset{u\in U}{max}H(x^0 (t),\psi^0 (t),u(t)),t\in \[0,t^* \[,$  где $\psi^0 (t),t\in T$ – решение сопряженной системы $\dot{\psi} =-(\partial H(x(t),\psi,u(t)))/\partial x$ с начальным условием$ \psi^0 (t^* )= -\partial \varphi(x^0 (t^* ))/\partial x -\sum_{(i=1)}^{m}\lambda_i^0   (\partial h_i (x^0 (t^* )))/\partial x$, в котором $\lambda_i^0=2\rho_i h_i x^0 (t^* ),i=1,...,m$.
Данные теоремы используются для решения задач оптимального управления, однако не всегда их использование является эффективным.
\paragraph{Метод динамического программирования}
Применяя метод динамического программирования, мы изучаем все поле оптимальных траекторий. Для того, чтобы сравнение было наглядным, мы воспользуемся ранее заданной задачей (3). Зафиксируем некоторый произвольный момент времени $t \in \[t_0,T\]$. Рассмотрим вспомогательную задачу управления на отрезке $[t ,T]$. Через $V(t,x)$ обозначим минимальное значение критерия качества во вспомогательной задаче при начальном условии $x(t) = x$, где $x$ – произвольный вектор из $R^n$. Мы можем предположить, что функция $V(t,x)$ удовлетворяет соотношениям:
\begin{center}
	$\partial V(t,x)/\partial t+\overset{u \in U}{min}f^{'} (t,x,u)  \partial V(t,x)/\partial x=0,$
\end{center}
\begin{flushright}
	$t_0\le t\le T,x\in R^n,$ \hspace{7cm}                                (4)
\end{flushright}
\begin{center}
	$V(T,x)=F(x).$
\end{center}
Решив данную задачу и определив $V(t,x)$ мы можем найти управление $u(t,x)$ из соотношения:
\begin{flushright}
	$\overset{u \in U}{min}f^{'}(t,x,u)  \partial V(t,x)/\partial x=f^{'}(t,x,u(t,x))  \partial V(t,x)/\partial x$ \hspace{4cm}       (5)
\end{flushright}
Возможность находить конкретно управление есть характерная черта метода динамического программирования. Она становится особенно важной в условиях отсутствия полной информации. 
При решении конкретных задач с помощью метода динамического программирования мы решаем нелинейное уравнение в частных производных (4), а так же дополнительно исследуем оптимальное управление, получаемое из уравнения (5).
\newpage
\section{Численные и программные методы решения ЗОУ}
\paragraph{Численные методы}
Здесь будем говорить о четырех численных методах решения задач оптимального управления.
\begin{itemize}
	\item Метод проекции градиента
	\item Метод сопряженного градиента
	\item Метод Ньютона
	\item Метод штрафных функций
\end{itemize}
Рассматривать каждый метод мы не будем, а сфокусируемся исключительно на первом
\subparagraph{Метод проекции градиента}
Опишем алгоритм решения для задачи $f(x) \rightarrow min, x \in X \subset R^n$, где $f(x)$ - непрерывно дифференцируема, а множество $X$ выпукло, замкнуто и ограничено.
Пусть задано начальное приближение $x^0 \in X$ и методом проекции градиента вычислено $x^k \in X$. Следующее приближение вычисляем по формуле
\begin{flushright}
	$x^{k+1}=P_X(x^k-\alpha_k\bigtriangledown f(x^k)), \alpha_k > 0,k=0,1,... \hspace{4cm}(1)$
\end{flushright} 
Сам же алгоритм метода проекции градиента основывается на следующих трех теоремах.\\
\textbf{Теорема 1.} Пусть точка $x^*$ есть точка локального минимума функции $f(x)$ на множестве $X$. Функция $f(X)$ предполагается непрерывно дифференцируемой, а множество $X$ выпуклым и замкнутым. Тогда для произвольного $\alpha \ge 0$ справедливо равенство
\begin{center}
	$x^*=P_X(x^*-\alpha \bigtriangledown f(x^*)).$
\end{center}
\textbf{Теорема 2.} Пусть функция $f(x)$ является выпуклой, непрерывно дифференцируемой, множество $X$ выпуклым и замкнутым. Точка $x^*$ есть точка локального минимума тогда и только тогда, когда для произвольного $\alpha >0$ справедливо равенство
\begin{center}
	$x^*=P_X(x^*-\alpha \bigtriangledown f(x^*)).$
\end{center}
В следующей теореме формулируются условия сходимости метода проекции градиента.\\
\textbf{Теорема 3.} Пусть в задаче $f(x) \rightarrow min, x \in X \subset R^n$ функция $f(x)$ непрерывно дифференцируема, ограничена снизу, и на множестве $X$ её градиент удовлетворяет векторному условиб Липшица с константой $L$, то есть
\begin{center}
	$\|\bigtriangledown f(x + \bigtriangleup x) - \bigtriangledown f(x)\| \le L\|\bigtriangleup x\|  x, x + \bigtriangleup x \in X.$
\end{center}
Тогда при любом начальном приближении $x^0$ имеет место соотношение
\begin{center}
	$\overset{k\rightarrow \infty}{\lim}\|x^{k+1}-x^{k}\|=0$
\end{center}
Если дополнительно предположить, что множество
\begin{center}
	$M(x^0)=\{x\in R^n |f(x)\le f(x^0)\}$
\end{center}
ограничено, то последовательность $\{x^k\}$ сходится к непустому множеству $S_* = \{x|x\in M(x^0),(\bigtriangledown f(x),y-x)\ge 0$ при всех $y \in X\}$ стационарных точек.\\
На основе данных трех теорем мы решаем задачи методом проекции градиента
\paragraph{Программные системы}
Сложная программная система, в частности, пакет
для решения задач оптимального управления,
имеет свое поле эффективной работы, которое, как
правило, характеризуется классом решаемых системой
задач и арсеналом применяемых ею методов.
Однако структурные особенности программной
системы, взаимосвязь и соответствие приближенных
методов, применяемых на разных этапах
поиска численного решения, а также способность
системы самостоятельно конструировать путь поиска
решения в зависимости от выявленных особенностей
решаемой задачи выгодно отличают
программную систему от теоретически описанного
класса задач и набора методов для поиска их
решения. Иерархическая структура вычислительной
схемы и соответствующая композиция методов,
применяемых на всех уровнях поиска численного
решения, являются основными характеристиками
эффективной программной системы.\\
Программная система с хорошим уровнем
интеллекта кроме широкого спектра методов,
как правило, содержит много различных эвристик
и логических алгоритмов для анализа
сложившихся ситуаций, которые позволяют
продолжить поиск оптимального управления и
даже при прекращении сходимости какого-то
метода. В этом случае может произойти переход
либо к другому методу, либо – к подпрограмме,
которая с помощью двойственного метода вычислит
оптимальную оценку невязки выполнения
условий оптимальности. Для дальнейшей
оптимизации программа может выбрать более
подходящий алгоритм или, как, например, в
задачах линейного программирования, перейти
к обновлению базиса, чтобы избавиться от
ошибок округления, накопившихся в ходе выполнения
большого количества итераций.\\